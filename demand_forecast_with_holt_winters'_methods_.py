# -*- coding: utf-8 -*-
"""Demand Forecast with Holt-Winters' Methods .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LUCnCMQf8Vb3knLiSKHE27qaLMI7AYWT

This notebook provides the "Summary", the "Explanation and visualization of the data", the "Discussion of Holt-Winters' Multiplicative and Additive Methods" and the "Working Program".

To run the working program, you can  go to the "Working Program" section directly without running the cells in the preceding sections.

## Summary

The objective of this project is to forecast material quantity required given 3 different parameters through user determined inputs. These 3 parameters are namely: Client designation, Material designation and forecast date. Using past data as supplied under provided file ‘APS1017 Order data for Project’, there have been 10 clients identified with 20 material sets. 

Furthermore, it will be beneficial to also be able to forecast aggregate demand for future planning regardless of client or material specification. Hence if client is not specified, forecast of aggregate demand of a single material will be returned. Similarly, if material designation is not specified, forecast of aggregate demand of a single client will be returned.

Prior to deciding what type of forecasting algorithm to use, a time-series analysis of the past data was carried out using the Augmented Dicky Fuller test and the visualisation of seasonal decomposition to observe suitable forecasting models to use, whether stationary or non-stationary. This was carried out under the 3 different user input scenarios (#1 order quantity per client, #2 order quantity per material and #3 order quantity per client per material).

Since the data contained both stationary and non-stationary trends, combined with the fact that only 2 different forecasting models should be run for all scenarios each, selection of forecasting algorithm uses Holt-Winters' Additive and Holt-Winters' Multiplicative model where the multiplicative model was taught in-course and the additive model provides adequate comparison which is discussed in further detail.

The program is able to effectively predict future demand to a certain degree, however since the data contains significant variation and randomness, perhaps there are better models which offer higher accuracy which can be explored in future projects.

## Import The Dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.holtwinters import ExponentialSmoothing

import warnings
warnings.filterwarnings('ignore')

# Import the dataset
df = pd.read_csv('/content/APS1017 Order data for Project.csv')

df['Date'] = pd.to_datetime(df['Date'])

df.sort_values(by='Date', inplace=True)

# Visualize the dataset
df.head()

"""## Explanation and visualization of the data

This section visualises the given data under the 3 different user input scenarios: #1 order quantity per client, #2 order quantity per material and #3 order quantity per client per material.

At the end of each scenario, a unit root test is carried out to better determine whether the data is stationary or not. The Augmented Dicky Fuller (ADF) is one of the more widely used unit root tests for determining strength of correlation of a time-series defined by trend. Visually, 4 plots for each single input has been carried out showing original, trend, seasonality and residuals, however, the usage of the null hypothesis can better interpret such results. The assumption is of the null and alternate hypothesis is defined:

Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure. 

Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.

To satisfy such requirements, p-values are used in conjunction to represent such hypothesis and have been shown prior to the ADF plots for each section. The p-values are defined as follows:

p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.

p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.

From the results, all the data is more or less evenly split between stationary and non-stationary time series and hence indicates that we have to pick 2 different forecasting methods that are able to cover both. 

As an extra precaution, a second round of ADF was carried out on the ‘residual’ data after the first ADF to confirm whether it has been run to a high degree of satisfaction. All ‘residual’ data indicates a P<0.05 which shows a good analysis of the first ADF.

Source:

https://machinelearningmastery.com/time-series-data-stationary-python/

https://towardsdatascience.com/time-series-decomposition-and-statsmodels-parameters-69e54d035453

### #1 Order Quantity Per Client
"""

# Aggregate the order quantity by client and fill the nan values with zeros
client_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Client', aggfunc=np.sum).fillna(0)

# Visualize client_table
client_table.head()

"""Some dates are missing from the dataset. Here we use interpolation to fill the missing dates' entries."""

# Fill the missing dates' entries by interpolation
client_table = client_table.reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Visualize client_table
client_table.head()

"""Now the missing dates are added."""

# Plot each client's time series
for i, col in enumerate(client_table.columns):
  plt.figure(figsize=(10,5))
  plt.plot(client_table.iloc[:,i])
  plt.title(col)
  plt.ylabel('Order quantity')
  plt.xlabel('Date')

"""It seems that most time series don't have a clear trend, and it's uncertain whether seasonal cycles exist or not. Therefore, we are going to run the ADF test to check for stationarity. A p-value > 0.05 suggests non-stationarity while p-value <= 0.05 infers a stationary series."""

def check_stationarity(combination, table):
  stationary = []
  non_stationary = []
  for i, col in enumerate(table):
    result = adfuller(table.iloc[:,i])
    if result[1] <= .05:
      stationary.append(col)
    else:
      non_stationary.append(col)
    print('p-value of the time series for {} {}: {}'.format(combination, col, result[1]))
  
  return stationary, non_stationary

# Run the ADF test to check for stationarity of the time series in client_table
client_stat, client_non_stat = check_stationarity(combination='client', table=client_table)

# Clients that produce stationary time series
print('Clients with stationary time series: {}'.format(client_stat))
# Clients that produce non-stationary time series
print('Clients with non-stationary time series: {}'.format(client_non_stat))

"""60% of the time series are staionary (p-value <= 0.05) while the remaining are non-stationary. Therefore, our forecasting methods should be adapted to both stationary or non-stationary time series."""

def decomposition(combination, table, model, freq):
  p_values = []

  for i, col in enumerate(table.columns):
    decomposition = sm.tsa.seasonal_decompose(x=table.iloc[:,i], model=model, freq=freq)
    trend = decomposition.trend
    seasonal = decomposition.seasonal
    residual = decomposition.resid

    #Plot the components
    plt.figure(figsize=(12,8))
    plt.subplot(411)
    plt.plot(table.iloc[:,i], label='Original', color="blue")
    plt.title(col)
    plt.legend(loc='best')
    plt.subplot(412)
    plt.plot(trend, label='Trend', color="blue")
    plt.legend(loc='best')
    plt.subplot(413)
    plt.plot(seasonal,label='Seasonality', color="blue")
    plt.legend(loc='best')
    plt.subplot(414)
    plt.plot(residual, label='Residuals', color="blue")
    plt.legend(loc='best')
    plt.tight_layout()

    residual.dropna(inplace=True)
    # Run the Augmented Dickey-Fuller test
    result = adfuller(residual)
    p_values.append(result[1])
    print('p-value of the residual for {} {}: {}'.format(combination, col, result[1]))
  
  print('Average p-value: {}'.format(np.mean(p_values)))

"""We assume a weekly cycle (7 days) for decomposition."""

# Decompose the time series of each client's order quantity
# We naively assume an additive model and set the freq paramenter as 7 (i.e. a weekly cycle)
decomposition(combination='client', table=client_table, model='add', freq=7)

"""The residual is what’s leftover after trends and seasonality are removed. Since the p-values for the residual in all time series are <= 0.05, the decompostion model we used (additive model with a weekly cycle) are good enough.

Hence, we suggest using Holt-Winters’ additive model as our first method to forecast the series. This model becomes a simply exponential smoothing model for stationary time series when both the trend and the seasonality factor drops out.

We will also use Holt-Winters' multiplicative model as our second model for comparison.

### #2 Order Quanitity Per Material
"""

# Aggregate the order quantity by material and fill the nan values with zeros
material_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Material', aggfunc=np.sum).fillna(0)

# Fill the missing dates' entries by interpolation
material_table = material_table.reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Visualize material_table
material_table.head()

# Plot material_table
for i, col in enumerate(material_table.columns):
  plt.figure(figsize=(10.0,5.0))
  plt.plot(material_table.iloc[:,i])
  plt.title(col)
  plt.ylabel('Order quantity')
  plt.xlabel('Date')

"""Similar to the client's order quantity, no clear trend and seasonality were observed, ADF test is carried out to check for stationarity."""

# Run the ADF test to check the stationarity of the time series in material_table
material_stat, material_non_stat = check_stationarity(combination='materials', table=material_table)

# Materials that produce stationary time series
print('Materials with stationary time series: {}'.format(material_stat))
# Materials that produce non-stationary time series
print('Materials with non-stationary time series: {}'.format(material_non_stat))

"""Again, both stationary and non-stationary time series exist."""

# Decompose the time series of each material's order quantity
# We naively assume an additive model and set the freq paramenter as 7 (i.e. a weekly cycle)
decomposition(combination='material', table=material_table, model='add', freq=7)

"""Again, both the Holt-Winters’ additive and multiplicative models can be used for forecasting.

### #3 Order Quantity Per Client Per Material
"""

# Aggregate order quantity by client and material and fill the nan values with zeros
combine_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client','Material'], aggfunc=np.sum).fillna(0)

# Fill the missing dates' entries by interpolation
combine_table = combine_table.reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Visualize the aggregated order quantity
combine_table.head()

# Plot combine_table
for i, col in enumerate(combine_table.columns):
  plt.figure(figsize=(10,5))
  plt.plot(combine_table.iloc[:,i])
  plt.title(col)
  plt.ylabel('Order quantity')
  plt.xlabel('Date')

"""No clear trend and seasonality are observed, we move on to run the ADF test."""

# Run the ADF test to check the stationarity of the time series in combine_table
combine_stat, combine_non_stat = check_stationarity(combination='client-material', table=combine_table)

# Client-Materials that produce stationary time series
print('Client-Materials with stationary time series: {}'.format(combine_stat))
# Client-Materials that produce non-stationary time series
print('Client-Materials with non-stationary time series: {}'.format(combine_non_stat))

"""Again, both stationary and non-stationary time series exist in separate client-material combinations."""

# Decompose the time series of each material's order quantity
# We naively assume an additive model and set the freq paramenter as 7 (i.e. a weekly cycle)
decomposition(combination='client-material', table=combine_table, model='add', freq=7)

"""The average p-value of the residuals is not as low as the client's and the material's ones. This can be explained by the fewer data points in specific client-material time series (actually we can't even treat them as normal time series if only a few data points exist, i.e. a client orders a certain material for only a few times). But all-in-all the decomposition still performs well, achieving an average p-value <= 0.05.

Hence, we can also adapt the Holt-Winters’ additive and multiplicative models for forecasting.

## Discussion of Holt-Winters' Multiplicative and Additive Methods

Reference: https://otexts.com/fpp2/holt-winters.html

### Holt-Winters' Multiplicative Model

This method was taught during class and was chosen as the components can be adjusted to represent both stationary and non-stationary datasets with the adjustment of trend and seasonal factors. Ultimately, by automatically fitting trend and seasonal factor components to drop out of the forecasting equation (depending on past data), the only smoothing equation of series represent a stationary forecast which is applicable with our mixed dataset. Winters' multiplicative holds the general equation illustrated below: 

Forecast:
$F_{t+1} = (S_t+G_t)c_{t+1-N}$

Series:
$S_t = \alpha(D_t / c_{t-N}) + (1-\alpha) (S_{t-1} + G_{t-1})$

Trend:
$G_t = \beta(S_t - S_{t-1}) + ( 1 -\beta)G_{t-1}$

Seasonal factor:
$C_t = \gamma(D_t / S_t) + (1 - \gamma)c_{t-N}$

Where the three smoothing equations combine multiplicatively with seasonal factor. The smoothing equations of St, Gt and Ct represent series, trend and seasonal factor respectively whereas Ft+1 indicates forecast for the t+1 day. This equation discounts the random component found in the base model equation. The series equation represents deasonalised and weighted (between observation and past data) as represented by alpha. Trend represents linear increase or decrease of the past dataset which uses series equation rather than observed data to decreases fluctuations. Seasonal factor of 7 days is used to predict adjustment depending on previous season observations whereas alpha, beta and gamma are all optimised by the function automatically.

One of the major limitations of this equation is identified when previous datapoints Dt which have no entry (NaN) are set to 0. St will also return 0 which produces a numerical error when calculating seasonal factor Ct as the denominator is 0. Moreover, this problem arises regardless of whether a dataset is stationary and is circumnavigated by performing ‘backfill’ of replacing the 0 datapoints with succeeding non-zero datapoint. It is understood that a very small number could also be fitted to approximate a zero value, however, it results in a seasonal factor Ct approaching infinity. The main issue still stands where a Winters' multiplicative model will require additional information handling when 0 quantity is demanded. 

With such limitation on zero values, using another model will be ideal to prevent force fitting certain arbitrary values other than 0.

### Holt-Winters' Additive Model

By using another method with similar components, we are able to compare and avoid previous problems of requiring backfill of values which does not logically reflect the real data. By replacing NaN values with 0, using Winters' Additive formulation bypasses all errors described above. The general formulation is illustrated below:

Forecast:
$F_{t+1} = S_t + G_t + c_{t+1-N}$

Series:
$S_t = \alpha(D_t / c_{t-N}) + (1 - \alpha)(S_{t-1} + G_{t-1})$

Trend:
$G_t = \beta(S_t - S_{t-1}) + (1 - \beta)(G_{t-1})$

Seasonal factor:
$c_t = \gamma(D_t - S_t) + (1 - \gamma)c_{t-N}$

There is no denominator in the Ct component which will require further investigation if 0 is inputted hence this presents a better fitting model. The 3 smoothing equations are exactly the same as described in the previous method except for Ct where the first y component is replaced by (Dt-St) and the forecast equation Ft+1 is wholly additive rather than multiplicative of the seasonal factor Ct.

### Comparison Of The Two Models

The two variations of Holt-Winters’ method vary how the seasonal component affects the baseline equation. Generally, the additive method yields better results when the seasonal variations are constant whereas the multiplicative method is better utilised when seasonal variations are altering proportionally to the level of the series. Source: https://kourentzes.com/forecasting/2014/11/09/additive-and-multiplicative-seasonality/

Below shows a better comparison between both methods and their base data entries by taking client c2 and material 11000851. It is observed that the multiplicative method provides a much better forecast (peaks and constant demands) with the backfilled observation data compared to the additive method that fills the NaN values with zeros.
"""

# Aggregate order quantity by client-material, fill the nan values with zeros and fill the missing dates' entries by interpolation
combine_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client', 'Material'], aggfunc=np.sum) \
    .fillna(0) \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Aggregate order quantity by client-material, backfill the nan values (forwardfill the last row) and fill the missing dates' entries by interpolation
combine_bf = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client', 'Material'], aggfunc=np.sum) \
    .fillna(method='bfill') \
    .fillna(method='ffill') \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

model1 = ExponentialSmoothing(combine_table[('c2', 11000851)], trend='add', seasonal='add', seasonal_periods=7)
model1_fit = model1.fit()

model2 = ExponentialSmoothing(combine_bf[('c2', 11000851)], trend='add', seasonal='mul', seasonal_periods=7)
model2_fit = model2.fit()

plt.figure(figsize=(10,5))
plt.plot(model1_fit.fittedvalues, linestyle='--', color='tab:purple', label='additive zero-filling')
plt.plot(model2_fit.fittedvalues, linestyle='--', color='tab:green', label='multiplicative backfilling')
plt.plot(combine_table[('c2', 11000851)], linestyle='-', label='original zero-filling')
plt.plot(combine_bf[('c2', 11000851)], linestyle='-', label='original backfilling')
plt.title(('c2', 11000851))
plt.legend()

"""Now we try to substitute the NaN values to be close to be a very small number decimal and try using the multiplicative method."""

# Aggregate order quantity by client and material and fill the nan values with a very small decimal number
combine_decimal = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client','Material'], aggfunc=np.sum) \
    .fillna(0.0000001) \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

combine_decimal.head()

model1 = ExponentialSmoothing(combine_table[('c2', 11000851)], trend='add', seasonal='add', seasonal_periods=7)
model1_fit = model1.fit()

model2 = ExponentialSmoothing(combine_decimal[('c2', 11000851)], trend='add', seasonal='mul', seasonal_periods=7)
model2_fit = model2.fit()

plt.figure(figsize=(10,5))
plt.plot(model1_fit.fittedvalues, linestyle='--', color='tab:purple', label='additive zero-filling')
plt.plot(model2_fit.fittedvalues, linestyle='--', color='tab:green', label='multiplicative decimal-filling')
plt.plot(combine_table[('c2', 11000851)], linestyle='-', label='original')
plt.title(('c2', 11000851))
plt.legend()

"""However, when we substitute the NaN values to be close to be a very small number decimal, it is clear that the additive one is the better fit because the multiplicative one (using very small decimal rather than 0) flattens out due to the seasonal factor becoming constant throughout all datasets as the ratio of Dt and St are similar (and hence smoothing out any seasonal jumps). 

Even though there is a good fit for the multiplicative method with backfilling, the backfill does not match the observed data provided and is hence not reflective of the real world and produces an over forecast. Overall, the additive method should be used due to the limitation of the multiplicative although the additive method is still generally unstable and is unable to match peaks like the multiplicative method does.

## Working Program

### Instructions

Please run the following cells before running the main program.

First, we need to upload the "APS1017 Order data for Project.csv" file and import our dataset.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.holtwinters import ExponentialSmoothing

import warnings
warnings.filterwarnings('ignore')

# Import the dataset
df = pd.read_csv('/content/APS1017 Order data for Project.csv')

df['Date'] = pd.to_datetime(df['Date'])

df.sort_values(by='Date', inplace=True)

# Visualize the dataset
df.head()

"""Then, we need to prepare aggregated order quantities for client, material and client-material by filling NaN values with zeros for the additive method."""

# Aggregate order quantity by client, fill the nan values with zeros and fill the missing dates' entries by interpolation
client_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Client', aggfunc=np.sum) \
    .fillna(0) \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Aggregate order quantity by material, fill the nan values with zeros and fill the missing dates' entries by interpolation
material_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Material', aggfunc=np.sum) \
    .fillna(0) \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Aggregate order quantity by client-material, fill the nan values with zeros and fill the missing dates' entries by interpolation
combine_table = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client', 'Material'], aggfunc=np.sum) \
    .fillna(0) \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

"""After that, we need to prepare aggregated order quantities for client, material and client-material by backfilling for the multiplicative method (MBF)."""

# Aggregate order quantity by client, backfill the nan values (forwardfill the last row) and fill the missing dates' entries by interpolation
client_bf = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Client', aggfunc=np.sum) \
    .fillna(method='bfill') \
    .fillna(method='ffill') \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Aggregate order quantity by material, backfill the nan values (forwardfill the last row) and fill the missing dates' entries by interpolation
material_bf = pd.pivot_table(df, values='Order Quantity', index='Date', columns='Material', aggfunc=np.sum) \
    .fillna(method='bfill') \
    .fillna(method='ffill') \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

# Aggregate order quantity by client-material, backfill the nan values (forwardfill the last row) and fill the missing dates' entries by interpolation
combine_bf = pd.pivot_table(df, values='Order Quantity', index='Date', columns=['Client', 'Material'], aggfunc=np.sum) \
    .fillna(method='bfill') \
    .fillna(method='ffill') \
    .reindex(pd.date_range('2019-12-05', '2020-02-24')).interpolate()

"""Moving on, run the following functions to set up our main program."""

def user_input():
  # Client input
  client = input('Select a client or type "none": ')
  # Reject invalid client input
  if client not in df.Client.unique() and client != "none":
    print('Invalid client input')
    return

  # Material input
  material = input('Select a material or type "none": ')
  # Reject invalid material input
  if material not in df.Material.unique().astype(str) and material != "none":
    print('Invalid material input')
    return
  if material != "none":
    material = int(material)
  
  # Reject non-existed client-material pair
  if (client, material) not in combine_table.columns and client != "none" and material != "none":
    print('Invalid client-material combination')
    return

  # Reject empty client and material inputs
  if client == "none" and material == "none":
    print('No data is selected')
    return

  # Date input
  try:
    date = pd.to_datetime(input('Select a future date in the format of "YYYY-MM-DD" (starting from 2020-02-25): '))
    # Reject invalid date input
    if date < pd.to_datetime('2020-02-25'):
      print('Invalid date input')
      return
  # Reject invalid date input
  except:
    print('Invalid date input')
    return

  # Calculate the days ahead 
  days_ahead = (date - pd.date_range('2019-12-05', '2020-02-24')[-1]) / np.timedelta64(1, 'D')

  return client, material, days_ahead

def exponential_smoothing_forecast(table, column, days_ahead, model_type):
  model = ExponentialSmoothing(table[column], trend='add', seasonal=model_type, seasonal_periods=7)
  fitted = model.fit()
  forecast = fitted.forecast(steps=days_ahead)

  plt.figure(figsize=(10,5))
  plt.title(column)
  plt.plot(table[column], label='original')
  plt.plot(fitted.fittedvalues, linestyle='--', color='red', label='model prediction')
  plt.plot(forecast, linestyle='--', color='orange', label='future forecast')
  plt.legend()

  print('The forecasted order quantity on {} is {}'.format(str(forecast.index[-1]).split()[0], forecast[-1]))

def forecast_by_condition_additive(client, material, days_ahead, model_type):
  if client == "none":
    exponential_smoothing_forecast(material_table, material, days_ahead, model_type)
  elif material == "none":
    exponential_smoothing_forecast(client_table, client, days_ahead, model_type)
  else:
    exponential_smoothing_forecast(combine_table, (client, material), days_ahead, model_type)

def forecast_by_condition_multiplicative(client, material, days_ahead, model_type):
  if client == "none":
    exponential_smoothing_forecast(material_bf, material, days_ahead, model_type)
  elif material == "none":
    exponential_smoothing_forecast(client_bf, client, days_ahead, model_type)
  else:
    exponential_smoothing_forecast(combine_bf, (client, material), days_ahead, model_type)

def run_main_program(model_type):
  try:
    client, material, days_ahead = user_input()
  except:
    return

  if model_type == 'add' or model_type == 'additive':
    forecast_by_condition_additive(client, material, days_ahead, model_type)
  elif model_type == 'mul' or model_type == 'multiplicative':
    forecast_by_condition_multiplicative(client, material, days_ahead, model_type)

"""### Run Main Program

Choose one of the two methods for forecasting.

#### Method 1: Holt-Winters' Additive Model

Example 1: Only the client is selected
"""

run_main_program(model_type='add')

"""Example 2: Only the material is selected"""

run_main_program(model_type='add')

"""Example 3: Both the client and the material are selected"""

run_main_program(model_type='add')

"""Now's your turn!

Please select a client from [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10].

Please select a material from [11000851, 11000858, 11001186, 12042081, 12160666, 12222902, 12246740, 12271469, 12293768, 12293783, 12320618, 12320619, 12320670, 12321895, 12321896, 12322779, 12365978, 12395525, 12396696, 12404641].

Please select a date starting from 2020-02-25.

Run the following cell again if invalid inputs are detected.
"""

run_main_program(model_type='add')

"""#### Method 2: Holt-Winters' Multiplicative Model

Example 1: Only the client is selected
"""

run_main_program(model_type='mul')

"""Example 2: Only the material is selected"""

run_main_program(model_type='mul')

"""Example 3: Both the client and the material are selected"""

run_main_program(model_type='mul')

"""Now's your turn!

Please select a client from [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10].

Please select a material from [11000851, 11000858, 11001186, 12042081, 12160666, 12222902, 12246740, 12271469, 12293768, 12293783, 12320618, 12320619, 12320670, 12321895, 12321896, 12322779, 12365978, 12395525, 12396696, 12404641].

Please select a date starting from 2020-02-25.

Run the following cell again if invalid inputs are detected.
"""

run_main_program(model_type='mul')